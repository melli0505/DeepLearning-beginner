# 1. Linear Regression

### 1) 선형회귀란?

$y(hat) = wx + b$

- 1차 함수를 이용하여 어떤 x가 주어졌을 때 y의 값을 예측하는 것.
- 이 식 자체가 모델.
- 산점도 그래프를 가장 잘 표현하는 직선의 방정식을 구하는 것이 회귀 알고리즘의 목표. ⇒ 그 중 하나가 gradient descent, 경사하강법

    ** 이외에도 정규방정식, 결정트리, 서포트 벡터머신 등이 있다.

### 2) 예측값으로 올바른 모델 찾기

1. w와 b(가중치와 절편) 초기화하기
2. x에서 샘플 하나를 무작위로 선택해 y hat 계산하기
3. y hat의 값과 실제 y 값 비교
4. y hat과 y의 오차가 줄어들도록 w, b 재조정
5. 모든 샘플 처리할 때까지 반복

- 가중치 업데이트

    w 업데이트에 따른 변화율을 계산해보면 결국 훈련데이터 샘플!

    y hat 값이 y보다 작으면 증가시켜야한다. (변화율이 양수) → 음수라면? w 값을 감소시켜면 된다. 양수와 음수를 번거롭게 비교하지 않고 하려면 변화율을 이용하면 된다.

    - 변화율로 가중치 업데이트하기.
        - 변화율이 0보다 큰 경우에 y_hat이 증가해야한다. ⇒ 변화율을 w에 더하는 방식으로 w 증가
        - 변화율이 0보다 작은 경우 y_hat을 증가시키려면? ⇒ 마찬가지로 더하면 됨.
    - 변화율로 절편 업데이트 하기
        - b에 관한 변화율을 구하여 변화율로 b 업데이트

    w_rate = (y hat inc - y hat) / (w inc - w) = ((x[0] * w inc + b) - (x[0] * w + b)) / (w inc - w) = (x[0] * (w + 0.1) - w) / (w + 0.1 - w ) = x[0]

    위 수식의 수치(0.1)은 임의로 설정한 변화량

    b_rate = (y hat inc - y hat) / (b inc - b) = ((x[0] * w+ b inc) - (x[0] * w + b)) / (b inc - b) = (b + 0.1 - b) / (b + 0.1 - b) = 1 

    1차함수에서 절편이 1 증가하면 그래프 위치가 y 방향으로 1 올라가니까 변화율 1

- 예시 1 - 당뇨병 환자 병 진전 정도 예측 모델
    - sklearn datasets - load_diabetes()
        - 10개의 특성 + 442개 sample로 구성됨.
        - 샘플 한 개에 target 하나가 대응됨 ⇒ target 갯수도 442개
        - 수치가 무슨 뜻인지는 의사가 알아야할 거임.
    - 데이터 시각화
        - matplotlib의 pyplot scatter로 산점도 생성

### 3) 손실 함수와 경사하강법의 관계

경사하강법 ⇒ 어떤 손실함수가 정의되었을 때 손실 함수 값이 최소가 되는 지점을 찾아가는 벙법.

앞의 방법은 제곱오차라는 손실함수 사용

제곱 오차  $SE = (y - y hat)^ 2$

제곱 오차 함수의 최솟값 ⇒ w를 변화시키면서 함수의 값이 낮은 쪽으로 이동

→ w = 기울기! 기울기를 구하기.

- 가중치에 대한 제곱오차 미분

    w에 관해서 편미분, (y hat = wx  + b 임에 주의)

    $∂SE/∂w = ∂/∂w( y - yhat)^2 = 2(y-yhat)(-∂/∂w * yhat) = 2(y - yhat)(-x) = -2(y-yhat)x$

    따라서 정리하면 

    $∂SE/∂w = -2x(y-yhat)$

    처음부터 제곱오차 공식을 1/2 해줬으면 더 깔끔했을 테니까 여기에서도 $-x(y-yhat)$ 으로 쓴다.

    가중치 업데이트

    $w = w - ∂SE/∂w = w - (-x(y - yhat))$ 

    변화율을 빼는 것은 손실함수의 아래쪽으로 이동하고 싶기 때문(최고차항이 양수인 n차 방정식 아래)

    err = y - y hat 이고 w_rate가 x[i]와 같았으므로 이 식은 앞선 w = w + w_rate * err와 같다.

    ```jsx
    y_hat = x_i * w + b
    err = y_i - y_hat
    w_rate = x_i
    w = w + w_rate * err
    ```

- 절편에 대한 제곱오차 미분

    *** 1/2를 곱한 제곱오차 식을 사용 ***

    $∂SE/∂b = ∂/∂b * 1/2(y - yhat)^2 = (y-yhat)(-∂/∂b  * yhat) = (y-yhat)(-1) = yhat - y$

    w와 같은 이유로 빼주면서 절편 업데이트

    $b = b - ∂SE/∂b = b  - (y-yhat)$

    ```jsx
    err = y_i - y_hat
    b = b - err
    ```